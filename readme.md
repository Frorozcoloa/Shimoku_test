# Ejecuci贸n del c贸digo

para ejecutar el c贸digo lo primero es instalar pip-tools

```
pip install pip-tools
```

compilamos los requierements

```
pip-compile requirements.in
```

instalamos los requirements generados

```
pip install -r requirements.txt
```

Servimos el mflow en el puerto 8004

```
mlflow server --host 127.0.0.1 --port 8084
```

ejecuatos todo el pipeline

```
python main.py
```

# Ingenier铆a de caracter铆sticas

## datasets offers.csv

En el proceso inicial del entrenamiento del modelo, se llev贸 a cabo la ingenier铆a de caracter铆sticas como primer paso. Para esto, se realizaron tres an谩lisis exploratorios con el objetivo de comprender las caracter铆sticas del negocio. En primer lugar, se examin贸 el conjunto de datos, realizando un an谩lisis detallado de cada columna y su posible relaci贸n con la variable predictora.

### Status

El enfoque inicial consisti贸 en analizar la variable objetivo, donde se procedi贸 a examinar todas las clases disponibles. Como resultado de este an谩lisis, se tom贸 la decisi贸n de mantener las clases "Closed Won" y "Closed Loss". Esta elecci贸n se bas贸 en la capacidad de estas clases para describir de manera precisa el objetivo del modelo, que es proporcionar un puntaje que var铆a entre 0 y 1. En este contexto, el valor 0 representa la probabilidad de perder un cierre, mientras que el valor 1 indica la probabilidad de lograr un cierre exitoso. Este enfoque busca optimizar la capacidad del modelo para realizar un scoring efectivo en la clasificaci贸n de oportunidades de negocio.

**Title: Distribuci贸n de la variable status**

![1702156015245](image/readme/1702156015245.png)

Tras realizar este proceso, se observ贸 que el 57.9% de las instancias pertenec铆an a la clase "Closed Won" y el 42.1% a la clase "Closed Loss". Este equilibrio en la distribuci贸n de clases indic贸 que no exist铆a un desbalanceo significativo en los datos. Con esta consideraci贸n, se procedi贸 a eliminar los datos relacionados con la otra clase, consolidando as铆 un conjunto de datos m谩s enfocado y representativo para el objetivo del modelo.

### Use Case

La segunda variable analizada fue "use_case". Este an谩lisis proporcion贸 una comprensi贸n m谩s profunda del caso de uso de la plataforma. Los resultados indicaron una fuerte asociaci贸n con eventos corporativos. En consecuencia, se tom贸 la decisi贸n de crear una variable binaria para clasificar los casos como eventos corporativos y eventos no corporativos. Esta nueva variable binaria se introdujo con el objetivo de capturar y reflejar de manera m谩s efectiva la naturaleza espec铆fica de los eventos corporativos en relaci贸n con el rendimiento del modelo.

**Title: Distribuci贸n de la variable use case**

![1702156529270](image/readme/1702156529270.png)

La evaluaci贸n de la variable "use_case" revel贸 que no exist铆a un desbalanceo significativo, y se destac贸 su predominante aplicaci贸n en eventos corporativos.

### Precio

Posteriormente, la atenci贸n se centr贸 en la variable de "precio". Durante el an谩lisis, se identificaron varios valores iguales a cero, sugiriendo que algunos clientes hab铆an obtenido servicios de manera gratuita. Aunque la distribuci贸n de esta variable mostraba un sesgo hacia la izquierda, se determin贸 que no ten铆a un impacto sustancial en la m茅trica de AUC, la cual se explicar谩 m谩s adelante por qu茅 fue seleccionada como m茅trica principal. Se realizaron pruebas con correcciones utilizando logaritmos y normalizaci贸n, sin embargo, se observ贸 que estas modificaciones no generaban cambios significativos en la m茅trica AUC.

**Title: Distribuci贸n de la variable precio**

![1702157284450](image/readme/1702157284450.png)

**Title: Distribuci贸n de la variable np.log(precio+1)**

![1702157341465](image/readme/1702157341465.png)

Se procedi贸 con un an谩lisis de las distribuciones de datos, tanto en su forma cruda como despu茅s de las transformaciones realizadas. Este an谩lisis permiti贸 visualizar la efectividad de las transformaciones aplicadas y su impacto en la estructura de los datos.

### Loss Reason

Adem谩s, se llev贸 a cabo un exhaustivo an谩lisis exploratorio de la nulidad de los datos. En particular, se examin贸 la variable "loss_reason", y se identific贸 una correlaci贸n de 1 mediante la correlaci贸n de Cram茅r con la variable "status". Esta alta correlaci贸n, respaldada por la naturaleza y los valores de la variable, sugiri贸 que "loss_reason" era un registro de las razones por las cuales un cliente no adquiri贸 el producto. Se realiz贸 un experimento entrenando modelos con esta variable, y se observ贸 que los modelos generaban m茅tricas del AUC cercanas a 0.99. Este hallazgo indic贸 la presencia de una fuga de datos (data leak) y, en consecuencia, se tom贸 la decisi贸n de eliminar la variable "loss_reason" del conjunto de datos.

Esta acci贸n se bas贸 en la necesidad de mantener la integridad del modelo y evitar la introducci贸n de informaci贸n sesgada o filtrada que pudiera afectar la generalizaci贸n del modelo en entornos del mundo real.

**Title: Heatmap de la matrix de correlaci贸n de variables categ贸ricas**

![1702157903119](image/readme/1702157903119.png)

Adicionalmente, se explor贸 la correlaci贸n de la variable "loss_reason" no solo con "status" sino tambi茅n con las variables "precio" y "discount_code". La correlaci贸n identificada con estas variables refuerza la evidencia de que "loss_reason" ten铆a informaci贸n cr铆tica para la predicci贸n del modelo y, por lo tanto, su eliminaci贸n fue justificada.

### Discount code

En el caso de "discount_code", se observ贸 que, aunque el c贸digo en s铆 mismo no aportaba informaci贸n 煤til, la presencia de valores nulos indicaba que el cliente no hab铆a recibido un c贸digo de descuento. Este hallazgo result贸 significativo, ya que se revel贸 que la existencia de un c贸digo de descuento. Como consecuencia, se transform贸 la variable "discount_code" en una variable binaria que indicaba la presencia o ausencia de un c贸digo de descuento, facilitando as铆 la interpretaci贸n y aplicaci贸n efectiva en el modelo.

**Title: Distribuci贸n de la variable has Discount code**

![1702158224356](image/readme/1702158224356.png)

### Open date y closed date

La siguiente fase del an谩lisis se centr贸 en las variables "open_date" y "closed_date". A partir de estas, se deriv贸 una variable adicional de gran relevancia: la duraci贸n. La duraci贸n se define como la diferencia en d铆as entre la fecha de cierre y la fecha de apertura de la oportunidad de negocio. Durante este proceso, se identificaron valores negativos, interpretados como el tiempo de duraci贸n de la oferta. Se consider贸 que estos datos podr铆an deberse a errores de digitaci贸n, dado que no se contaba con informaci贸n sobre la forma de adquisici贸n de los datos.

Aunque en una etapa inicial no se tiene informaci贸n sobre el tiempo real de negociaci贸n, la creaci贸n de la variable de duraci贸n permitir谩 realizar an谩lisis posteriores para comprender si los clientes requieren tiempo adicional para entender el producto o adquirir m谩s informaci贸n. Este an谩lisis tambi茅n podr铆a ayudar a determinar si es posible llevar a cabo una negociaci贸n de manera inmediata. En esencia, se trata de alinear el horario del equipo comercial con las necesidades y comportamientos de los clientes.

**Title: Distribuci贸n de la variable duraci贸n**![1702159214077](image/readme/1702159214077.png)

Al explorar la distribuci贸n de la variable de duraci贸n, se observ贸 que la m茅trica se ubicaba alrededor de 0, con valores comprendidos principalmente entre -500 y 500. Esta caracter铆stica refuerza la hip贸tesis de que los valores negativos podr铆an deberse a errores de digitaci贸n o a una interpretaci贸n inadecuada de los datos. La presencia de valores centrados en torno a 0 indica que, en su mayor铆a, las oportunidades de negocio ten铆an una duraci贸n relativamente corta o nula, lo cual puede ser atribuible a la inmediatez en las transacciones o a la falta de informaci贸n adicional.

A pesar de la incertidumbre inicial sobre la autenticidad de estos valores, el an谩lisis posterior demostr贸 que la variable de duraci贸n desempe帽aba un papel crucial en la mejora de las m茅tricas del modelo. Esta informaci贸n respalda la decisi贸n de retener la variable, incluso con la presencia de valores at铆picos, ya que contribuye significativamente a la capacidad predictiva del modelo en cuanto al cierre exitoso de oportunidades de negocio.

### Pain

La variable "Pain" sigui贸 un proceso similar en t茅rminos de transformaci贸n. Se decidi贸 convertirla en una variable booleana para simplificar su representaci贸n. En esta nueva formulaci贸n, se asign贸 el valor "True" a las instancias que involucraban operaciones relacionadas con "Pain", mientras que cualquier otro valor se etiquet贸 como "False".

**Title: Distribuci贸n de la variable Pain**

![1702161401477](image/readme/1702161401477.png)

## Dataset lead

Para los datos externos, se procedi贸 a fusionar los conjuntos de datos "lead.csv" y "offers", creando as铆 un dataset m谩s compacto. Este nuevo conjunto de datos fue sometido a un an谩lisis detallado de cada variable, destacando tres de especial importancia: ciudad, Acquisition Campaign, Source y Created Date. Dado el tama帽o reducido del conjunto de datos, se evalu贸 la relevancia de agregar estas variables como caracter铆sticas, considerando que su contribuci贸n podr铆a ser insignificante en comparaci贸n con otras variables.

Para abordar este desaf铆o, se asign贸 a cada variable informaci贸n sobre su nulidad, especificando si la oferta proven铆a de una campa帽a de adquisici贸n, si estaba asociada a una ciudad o si ten铆a un objetivo claro. Respecto a "Created Date", se llev贸 a cabo una transformaci贸n restando su valor al de "open_date". Esta operaci贸n gener贸 una nueva variable que indicaba el tiempo transcurrido desde que un cliente potencial se convirti贸 en cliente objetivo, o si el valor era negativo, desde que se le realiz贸 la oferta hasta que se volvi贸 cliente potencial.

Esta estrategia permiti贸 maximizar la utilidad de los datos externos, incorporando informaci贸n valiosa de manera efectiva en el modelo de entrenamiento. La interpretaci贸n de estas variables complementarias se ajust贸 para adaptarse al contexto de la predicci贸n de cierres exitosos de oportunidades de negocio.

Entre las variables externas analizadas, se identific贸 que la m谩s relevante para el modelo era la que indicaba si la oferta proven铆a de un evento corporativo.

**title: Heatmap de correlaci贸n de variables categ贸ricas del la uni贸n de los dos datasets**

![1702162630705](image/readme/1702162630705.png)

Recordemos que se cre贸 un dataset m谩s peque帽o.

## Conclusi贸n

En conclusi贸n, el an谩lisis detallado del primer conjunto de datos revel贸 que la informaci贸n 煤nica contenida en este conjunto fue fundamental para la identificaci贸n de las variables m谩s importantes para el modelo. La ingenier铆a de caracter铆sticas realizada en este conjunto permiti贸 destacar aspectos cruciales, como la presencia de c贸digos de descuento, la duraci贸n de las oportunidades de negocio, y la distinci贸n entre ofertas asociadas a eventos corporativos y otras fuentes.

Aunque se incorporaron datos externos para enriquecer la informaci贸n, fue el conjunto de datos original el que proporcion贸 las variables m谩s influyentes para la predicci贸n del modelo. Este descubrimiento subraya la importancia de realizar un an谩lisis minucioso de los datos disponibles y resalta c贸mo la comprensi贸n profunda de las caracter铆sticas espec铆ficas de los clientes y las oportunidades puede ser clave para el 茅xito del modelo de predicci贸n. En adelante, la implementaci贸n de estas variables fundamentales en el modelo deber铆a mejorar significativamente su capacidad para prever el cierre exitoso de oportunidades de negocio.

Todo el an谩lisis e informaci贸n que incontraba, lo llevaba en un tabler贸 de miro. Es muy visual y ayuda al equipo a unir y dejar preguntas

[Whiteboard-miro](https://miro.com/app/board/uXjVMflpEIs=/?share_link_id=43714105302)

# Pipelines de preprocesamiento

Dentro de la estructura del proyecto, se han dise帽ado cuatro pipelines que siguen el concepto ETL (Extract, Transform, Load). Cada uno de estos pipelines se encarga de manejar una etapa espec铆fica del proceso, garantizando as铆 la modularidad y la flexibilidad en la ejecuci贸n del flujo de trabajo. Los pipelines se encuentran organizados en la carpeta "src" del proyecto, como se detalla a continuaci贸n:

```

 src
     integrated.py
     join_datasets.py
     preprocessing.py
     train.py
     utils.py
```

1. **preprocessing.py:** Este pipeline se encarga de realizar el preprocesamiento del dataset "offers.csv". Lee el conjunto de datos desde la ruta "datasets/raw/offers.csv" y guarda la versi贸n transformada en "datasets/processed/offers.csv". Aqu铆 se aplican las distintas transformaciones y manipulaciones de variables necesarias para preparar los datos para el entrenamiento del modelo.
2. **integrate_datasets.py:** Este pipeline se enfoca en el preprocesamiento del dataset "lead.csv". Similar al pipeline de "preprocessing.py", toma el dataset desde "datasets/raw/lead.csv" y guarda la versi贸n transformada en "datasets/processed/lead.csv". Se aplican las transformaciones espec铆ficas a este conjunto de datos.
3. **join_datasets.py:** Este pipeline se encarga de la integraci贸n de los dos datasets preprocesados, "offers.csv" y "lead.csv". Lee ambos conjuntos de datos desde sus rutas procesadas respectivas y los une en un 煤nico dataset. El dataset integrado se guarda en "datasets/processed/integrated_dataset.csv".
4. **train_model.py:** Este pipeline representa la etapa de entrenamiento del modelo. Utiliza el dataset integrado generado por el pipeline anterior y realiza el entrenamiento del modelo. Los modelos entrenados se almacenan para su uso posterior en el proceso de predicci贸n.

Esta organizaci贸n modular y la ubicaci贸n de los datasets en carpetas espec铆ficas facilitan la integraci贸n del proyecto con administradores de tareas como Apache Airflow o prefect, permitiendo la automatizaci贸n y programaci贸n del flujo de trabajo. Adem谩s, la estructura del proyecto facilita su mantenimiento y escalabilidad a medida que se incorporan m谩s funciones y se expande el alcance del proyecto.

Cada uno de los pipelines anteriormente mencionados crea autom谩ticamente un informe que se almacena en la carpeta "reports".

```

 reports

     integrated.html

     leads.html

     offer.html

```

Tambi茅n, cada pipeline cuenta con su propio conjunto de pruebas unitarias, siguiendo una filosof铆a de MLOps de integraci贸n continua.

```

 test

     datasets

     test_integrated.py

     test_preprocessing.py

     test_train.py

     tets_join_datasets.py

     __init__.py

```

## Entranamiento del modelo

El entrenamiento parte del hecho de que contamos con un conjunto de datos llamado "integrated" dentro de la carpeta "datasets/processes". Para abordar los datos nulos, se realizar谩 una imputaci贸n utilizando la moda en el caso de variables categ贸ricas y la media si son num茅ricas.

El proceso de entrenamiento constar谩 de dos fases, ambas llevadas a cabo con el framework PyCaret. Este framework ofrece una amplia integraci贸n con diversos modelos y proporciona orientaci贸n sobre qu茅 modelo utilizar en funci贸n de la m茅trica seleccionada. En este caso, la m茅trica elegida es el AUC (Area Under the Curve), ya que se busca que el modelo pueda identificar tanto la clase 0 como la clase 1, siendo este el objetivo del programa.

Para orquestar todo el proceso, se emplea MLflow. La finalidad es tener un repositorio que muestre los diferentes experimentos realizados y que siempre est茅 disponible el mejor modelo para su uso, ya sea a trav茅s de una API u otros medios. Todas las variables categ贸ricas se convertir谩n a datos num茅ricos mediante label encoding, y de manera interna, PyCaret realizar谩 la conversi贸n a one-hot encoding.

Este enfoque integrado y automatizado proporciona una base s贸lida para la creaci贸n y selecci贸n de modelos, garantizando que se utilice el mejor modelo entrenado para el cumplimiento del objetivo del programa. La implementaci贸n de MLflow facilita la gesti贸n y rastreo de experimentos, lo que contribuye a la transparencia y reproducibilidad de los resultados obtenidos en el proceso de entrenamiento.

PyCaret nos proporcionar谩 un modelo previamente entrenado con diversas arquitecturas. Esta caracter铆stica nos brinda la flexibilidad de contar con un modelo listo para la producci贸n, aline谩ndonos con una metodolog铆a 谩gil. Posteriormente, se llevar谩 a cabo un ajuste de los hiperpar谩metros del modelo con el fin de seleccionar los valores 贸ptimos. Es importante destacar que todo este proceso se realiza utilizando cross-validation de 10 pliegues, lo cual contribuye a evitar el sobreajuste del modelo.

Este enfoque garantiza que el modelo est茅 bien generalizado y pueda realizar predicciones robustas en datos nuevos. La elecci贸n cuidadosa de los hiperpar谩metros a trav茅s del ajuste y la validaci贸n cruzada mejora la capacidad del modelo para adaptarse a diferentes conjuntos de datos, aumentando su rendimiento en condiciones del mundo real.

### M茅tricas de experimento

Cabe resaltar que todos los experimentos dan una mayor m茅trica GradientBoostingClassifier

Un primer experimento est谩 plasmado en la carpeta notebooks 1-1 train_all.py

**title: Roc for train model**

![1702176850977](image/readme/1702176850977.png)

podemos ver que obtenemos una m茅trica de un AUC = 0.88, algo bueno para nuestro modelo

**Title: Importancia de las variables del modelo**

![1702176827359](image/readme/1702176827359.png)

Tambi茅n vemos las variables de mayor importancia, las cuales son has_discount_code, duration y price.

Veamos las m茅tricas con el dataset de testeo.

**Title: Curva ROC en testeo**

![1702176941788](image/readme/1702176941788.png)

Como se puede observar, tanto el AUC de entrenamiento con cross-validation (cv=10) como el AUC de prueba son iguales. Este hecho refleja aspectos cruciales del rendimiento del modelo, ya que demuestra su capacidad para diferenciar eficazmente entre las dos clases objetivo

**Matriz de confusi贸n en testeo**

![1702177412400](image/readme/1702177412400.png)

Todos los experimentos se encuentran en la carpeta notebooks.

 notebooks

     1-1  train_all.ipynb

     1-2 train_accurracy.ipynb

     1-3  train_drop_isnull.ipynb

     logs.log

En esta fase, se llevaron a cabo tres experimentos distintos. En el primer experimento, se introdujo una nueva columna de caracter铆sticas indicando si los datos son artificiales.

Los experimentos 1.1 y 1.3 mostraron similitudes significativas en t茅rminos de m茅tricas de rendimiento. Sin embargo, al examinar las curvas de aprendizaje, se identific贸 una tendencia preocupante en los modelos de tipo 谩rbol, espec铆ficamente en el Gradient Boosting Classifier (GBC), hacia el sobreajuste. Este fen贸meno se manifiesta de manera m谩s evidente en el GBC.

![1702178287828](image/readme/1702178287828.png)

![1702178325530](image/readme/1702178325530.png)

En vista de los resultados obtenidos y la observaci贸n de tendencias de sobreajuste en modelos de tipo 谩rbol, se ha tomado la decisi贸n de seleccionar el **LGBMClassifier** como el modelo preferido. Este modelo ha demostrado un rendimiento s贸lido y ha evitado los desaf铆os asociados con el sobreajuste que se observaron en otros modelos, especialmente en el Gradient Boosting Classifier (GBC).

La elecci贸n del LGBMClassifier se respalda no solo en su rendimiento general, sino tambi茅n en su capacidad para manejar eficientemente conjuntos de datos m谩s grandes y complejos, as铆 como su eficacia en t茅rminos de tiempo de entrenamiento. Este modelo proporciona una combinaci贸n equilibrada de velocidad y rendimiento, lo que lo hace adecuado para su implementaci贸n en entornos de producci贸n.

### M茅tricas del modelo 贸ptimo

**Title: Roc Curver en entrenamiento con cv=10**

![1702178862782](image/readme/1702178862782.png)

**Title: Variables importantes**

![1702178891063](image/readme/1702178891063.png)

**Title: Matriz de confuci贸n con los datos de testeo**

![1702178933264](image/readme/1702178933264.png)

**Title: Roc Curve con los datos de testeo**

![1702178946399](image/readme/1702178946399.png)

### Conclusi贸n

En resumen, observamos que nuestra m茅trica de inter茅s, el AUC, tanto en la fase de entrenamiento como en la de prueba, muestra similitudes significativas. Este resultado indica que el modelo seleccionado, el LGBMClassifier.

Adem谩s, se destaca la relevancia de la variable "duration". Aunque inicialmente no disponemos de informaci贸n sobre la duraci贸n de la oferta, esta variable ha demostrado ser crucial en el proceso de modelado. La capacidad para analizar la duraci贸n ofrece un margen de maniobra valioso para comprender el comportamiento del cliente y determinar el momento 贸ptimo para cerrar una oferta. Esta flexibilidad en el tiempo de permanencia de la oferta abierta agrega un elemento estrat茅gico y adaptativo al proceso de toma de decisiones.

# Continuaci贸n

1. **Creaci贸n del Pipeline de Implementaci贸n Continua (CD):**
   * Se proceder谩 a desarrollar un pipeline de implementaci贸n continua que permita el despliegue autom谩tico del modelo en entornos de producci贸n. Este proceso garantizar谩 que el modelo seleccionado est茅 disponible y listo para su uso de manera eficiente.
2. **Monitoreo Continuo del Modelo:**
   * La fase de monitoreo constante ser谩 una parte integral del proceso. Se llevar谩n a cabo pruebas regulares de KS-divergente para identificar posibles cambios en la distribuci贸n de datos y se帽ales de data drift. La predicci贸n de m茅tricas futuras contribuir谩 a una mejor comprensi贸n del rendimiento del modelo a lo largo del tiempo, facilitando la toma de decisiones informadas sobre la necesidad de reentrenamiento con nuevos datos.
3. **Utilizaci贸n de Frameworks como Evidently y Grafana:**
   * Se emplear谩n herramientas como el framework Evidently y Grafana para facilitar el monitoreo del modelo. Evidently permitir谩 realizar an谩lisis detallados de la calidad del modelo, mientras que Grafana ofrecer谩 una interfaz visual para el seguimiento y la visualizaci贸n de m茅tricas clave. Esta combinaci贸n de herramientas mejorar谩 la capacidad para detectar patrones inesperados o problemas de rendimiento, permitiendo ajustes y mejoras proactivas en el modelo.

Este enfoque integral garantizar谩 no solo la implementaci贸n exitosa del modelo en producci贸n, sino tambi茅n su mantenimiento y mejora continua en respuesta a los cambios en los datos y en el entorno del problema. La atenci贸n constante al monitoreo y la adaptabilidad son esenciales para asegurar que el modelo siga siendo efectivo a lo largo del tiempo.

# Vista del usario final

Al final el usario ver谩 un dashboard con metadatos e informaci贸n de la predici贸n de Shimoku.

al principio el usuario va a ver las predicciones positivas totales

![1702323971645](image/readme/1702323971645.png)

Una gr谩fica de pie sobre la informaci贸n de las etiquetas predichas

![1702324016908](image/readme/1702324016908.png)


Una tabla con el id, la predicci贸n y la variable m谩s importante

![1702324055646](image/readme/1702324055646.png)

y por ultimo el usario ver谩 las carecteristicas m谩s importantes

![1702324094369](image/readme/1702324094369.png)

La idea es que dentro de Shimoku, haya otro dashboard para el equipo interno. Por lo que vi leyendo la documentaci贸n, est谩 muy pensado para inteligencia de negocio y usarios finales, por eso es que no quise incluir los outputs de los pipelines. Igualmente por buena pr谩ctica es bueno que se hecha pero un dashboard interno.
